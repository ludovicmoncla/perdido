{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YrOKr9pwkxJw"
   },
   "source": [
    "# Creating Geospatial Data from Historical Texts in French\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://github.com/ludovicmoncla/perdido/blob/main/notebooks/perdido-geoparser-GeoExT-ECIR23.ipynb) [![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/ludovicmoncla/perdido/main?filepath=notebooks/perdido-geoparser-GeoExT-ECIR23.ipynb)\n",
    "\n",
    "This notebook is proposed by [L. Moncla](https://ludovicmoncla.github.io/) and [M. Gaio]().\n",
    "\n",
    "## Cite this work\n",
    "> Moncla, L. and Gaio, M. (2023). Perdido: Python library for geoparsing and geocoding French texts. In proceedings of the First International Workshop on Geographic Information Extraction from Texts (GeoExT), ECIR conference, Dublin, Ireland.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 1. Overview\n",
    "\n",
    "\n",
    "In this tutorial, we'll learn about a few different things:\n",
    "\n",
    "\n",
    "- Load a dataset from the `Perdido` library as a Python dataframe (articles from Diderot and d'Alembert's *Encyclopédie*)\n",
    "- Load data from TEI-XML files into a Python dataframe\n",
    "- Use a dataframe for simple data analysis\n",
    "- Use the `Perdido Geoparser` library for geoparsing French texts (geotagging + geocoding)\n",
    "  - Display geotagging results\n",
    "  - Map geocoding results\n",
    "- Compare `Perdido` NER results with `spaCy` and `Stanza` (python libraries)\n",
    "- Reflect on the limits of geoparsing historical French (and multilingual) texts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5gi1PFqtkxJy"
   },
   "source": [
    "## 2. Introduction\n",
    "\n",
    "Geoparsing (also known as toponym resolution) refers to the process of extracting place names from text and assigning geographic coordinates to them.\n",
    "This involves two main tasks: geotagging and geocoding.\n",
    "Geotagging consists to identify spans of text referring to place names while geocoding (or toponym resolution) consists to find unambiguous geographic coordinates.\n",
    "\n",
    "Geographic text analysis research in the digital humanities has focused on projects analyzing modern English-language corpora. \n",
    "In this tutorial we propose to highlight the difficulties of extracting and mapping geographical information from historical French texts.\n",
    "As we'll see in the following, in addition to the problem of language when it comes to historical documents, the early-modern period lacks temporally appropriate gazetteers.\n",
    "\n",
    "> McDonough, K., Moncla, L., & van de Camp, M. (2019). Named entity recognition goes to old regime France: geographic text analysis for early modern French corpora. International Journal of Geographical Information Science, 33, 2498–2522.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 The Perdido Geoparser python library\n",
    "\n",
    "[Perdido](https://github.com/ludovicmoncla/perdido) is a python text geoparser. It provides NLP and GIS methods for geoparsing French texts.\n",
    "It has initially been developed as a REST API for extracting and retrieving displacements from French hiking descriptions, during the [PERDIDO](http://erig.univ-pau.fr/PERDIDO/) and [ANR Choucas](http://choucas.ign.fr) projects.\n",
    "\n",
    "More recently, as part of the [GEODE project](https://geode-project.github.io) we have developed a custom version for historical documents and more specifically for the Encyclopédie.\n",
    "\n",
    "\n",
    "In this tutorial we'll see how to use the `Perdido` python library for geoparsing French texts. \n",
    "We will apply geoparsing on volume 7 of Encyclopedie corpus version released by the [ARTFL project](https://encyclopedie.uchicago.edu/) and we'll show the limits of geotagging and geocoding historical documents.\n",
    "\n",
    "### 2.2 Acknowledgement\n",
    "\n",
    "Data courtesy the [ARTFL Encyclopédie Project](https://artfl-project.uchicago.edu/), University of Chicago.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setting up the environment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Install python packages\n",
    "\n",
    "* If you already configured your environment using conda (`environment.yml`) or pip (`requirements.txt`), you can skip this step and go to section [3.2 Import the libraries](3.2-Import-the-libraries).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install perdido==0.1.28\n",
    "!pip install stanza==1.4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Import the libraries\n",
    "\n",
    "First, we will load some specific libraries from `Perdido` that we will use in this notebook. Next, we import some tools that will help us parse and visualize the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from perdido.geoparser import Geoparser\n",
    "from perdido.geocoder import Geocoder\n",
    "from perdido.datasets import load_edda_artfl, load_edda_perdido\n",
    "from spacy import displacy\n",
    "\n",
    "import os\n",
    "import lxml.etree as etree\n",
    "import xml.dom.minidom as xml\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b9ZN6YgcnxdK"
   },
   "source": [
    "\n",
    "## 4. Getting started\n",
    "\n",
    "In this notebook, we'll test out some basic queries of the *Encyclopédie* articles from volume 7 (H - Itzehoa, published in 1765). You can learn more about the other volumes [here](https://encyclopedie.uchicago.edu/node/102).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 4.1 Loading the ARTFL *Encyclopédie* dataset from the Perdido library\n",
    "\n",
    "Now we will see how we can load the dataset directly from the Perdido library. \n",
    "\n",
    "The next cell loads the data from the Perdido library, defines the data as `dataset`, and shows you the top 5 records (using the [head()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html) method from the [Pandas](https://pandas.pydata.org/docs/index.html) library). The data has been saved as a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the load_edda_artfl function to get the dataset\n",
    "d = load_edda_artfl()\n",
    "\n",
    "# The load_edda_artfl function returns a Python dictionnary, containing the dataset as a dataframe in the data entry\n",
    "dataset = d['data']\n",
    "\n",
    "# Display the 10 first row of the dataframe\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Exploring the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have access to all the attributes and methods of the [dataframe](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) object. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, we can print the number of rows in our dataframe which corresponds to the number of articles in our corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = dataset.shape[0]\n",
    "print('There are ' + str(n) + ' articles in the dataset.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to display and search the full text content of the articles stored in the dataframe. \n",
    "\n",
    "* Show full text for a specific article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.loc[dataset['head'] == 'FRONTIGNAN'].text.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More examples of how you can search and filter the dataframe structure are given in Section 9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nTcr95AokxKh"
   },
   "source": [
    "## 5. Perdido Geoparser\n",
    "\n",
    "\n",
    "In Natural Language Processing (NLP), the main first steps before processing text content consist in tokenizing sentences and words and assigning to each word its grammatical category (Part-of-Speech). \n",
    "\n",
    "This allows the construction of more complex rules or queries compared to a simple keyword search. E.g. we would know that \"city\" is a noun, and we can perform a search for all nouns in the corpus.\n",
    "\n",
    "\n",
    "These preprocessing steps are language dependent, and therefore we have to choose the right tool according to the language, style and period of our documents. This is a major difficulty when dealing with historical or ancient texts. For instance, for French it is difficult to find a POS tagger for pre-20th century French as major well known taggers are trained on contemporary corpora.\n",
    "\n",
    "> McDonough, K., Moncla, L., & van de Camp, M. (2019). Named entity recognition goes to old regime France: geographic text analysis for early modern French corpora. International Journal of Geographical Information Science, 33, 2498–2522.\n",
    "\n",
    "For now, the `Perdido` geoparser uses [Treetagger](https://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/) for preprocessing (tokenzation, lemmatization and part-of-speech tagging). \n",
    "\n",
    "\n",
    "The geotagging step of `Perdido` is perform using a cascade of finite-state transducers defining specific patterns for NER and identification of geographic information (spatial relations, etc.). \n",
    "> Mauro Gaio and Ludovic Moncla (2019). “Geoparsing and geocoding places in a dynamic space context.“ In The Semantics of Dynamic Space in French: Descriptive, experimental and formal studies on motion expression, 66, 353.\n",
    "\n",
    "Geoparsing in the digital humanities began with projects analyzing modern English-language corpora. But now, many researchers are developing projects for automatically identifying and geolocating places named in texts of many languages. \n",
    "\n",
    "Here we highlight the difficulties of extracting and mapping geographical information from historical French texts. In addition to language-related problems which impact the quality of tokenization, POS tagging, and NER, geocoding presents its own challenges. Once place names have been identifed in a text, correctly associating geographical coordinates with that place is a challenge. **Gazetteers** are knowledge bases that help researchers link place names with information about place, including its location. \n",
    "\n",
    "For our custom version of the `Perdido` Geoparser, the geocoding task uses a simple gazetteer lookup method. Several gazetteers can be used:\n",
    " - Nominatim (ie, OpenStreetMap) by default, \n",
    " - Geonames, \n",
    " - World Historical Gazetteer, \n",
    " - IGN (French National Institute of Geographic Information)\n",
    "\n",
    "Like using the most appropriate POS tagger, finding the best gazetteer for your corpus can be challenging. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Getting started with `Perdido`\n",
    "\n",
    "* Get the content from the article 'FRONTIGNAN' ([https://artflsrv03.uchicago.edu/philologic4/encyclopedie1117/navigate/7/1002/](https://artflsrv03.uchicago.edu/philologic4/encyclopedie1117/navigate/7/1002/)) from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = dataset.loc[dataset['head'] == 'FRONTIGNAN'].text.item()\n",
    "content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create a Geoparser object from the `Perdido` library. Specify that we are working with the *Encyclopédie* version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geoparser = Geoparser(version=\"Encyclopedie\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now you can use this geoparser for geoparsing text content. Let's try with the `content` variable that we declared before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = geoparser(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The geoparser return a `Perdido` object. This object has several attributes and methods. We'll now see some of them.\n",
    "\n",
    "* Accessing the XML-TEI result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.tei"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can use the `xml` library to get something easier to read:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xml.parseString(doc.tei).toprettyxml(indent=' ')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Accessing the geojson results generate during the geocoding phase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.geojson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Transform the Perdido object into a dataframe (only some of the attributes are kept):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = doc.to_dataframe()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Display named entities\n",
    "\n",
    "Often, it is useful to vizualize the output in sentence form. The `spacy` library provides a useful tool for this: `displacy`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc.to_spacy_doc(), style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc.to_spacy_doc(), style=\"span\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B7bw9rWGkxKx"
   },
   "source": [
    "### 5.3 Map place names\n",
    "\n",
    "\n",
    "For many projects, it is important to view the results of geoparsing on a map. Here, we can see the results plotted on a map. But remember, these are only the results for which coordinates could be found. Results that could not be matched to records in the gazetteer will not be mapped.\n",
    "\n",
    "* Here we see the geocoding results for 'Frontignan' mapped:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Ay5XDTM1kxKy",
    "outputId": "93df6728-dd7e-40c7-c2c5-fae62699a7c0"
   },
   "outputs": [],
   "source": [
    "doc.get_folium_map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Save/export the results in external files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.to_xml('FRONTIGNAN-perdido.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.to_geojson('FRONTIGNAN-perdido.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.to_csv('FRONTIGNAN-perdido.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Try another example\n",
    "\n",
    "GESSORIACUM - https://artflsrv03.uchicago.edu/philologic4/encyclopedie1117/navigate/7/2046/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = dataset.loc[dataset['head'] == 'GESSORIACUM'].text.item()\n",
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "doc = geoparser(content)\n",
    "displacy.render(doc.to_spacy_doc(), style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.get_folium_map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparison to other NER tools\n",
    "\n",
    "`Perdido` is a custom geoparsing library for French language documents. How does it compare to other state-of-the-art libraries?\n",
    "\n",
    "Comparing `Perdido`, `SpaCy`, and `Stanza` outputs, even for just 1-2 articles from our corpus, allows us to see common errors. One library might excel at identifying people, but struggle with complex place names. Another is better at capturing places named within phrases, but mixes up people and places. It is important to test multiple geoparsers for your corpus, and to understand how they can be adapted, in order to get the best results.\n",
    "\n",
    "### 6.1 SpaCy\n",
    "\n",
    "[SpaCy](https://spacy.io/) is a commonly-used NLP library that supports documents in many languages. `SpaCy` uses Machine Learning to perform NER (versus being a rule-based system)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Install the `spaCy` french pre-trained language model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download fr_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Import the `spaCy` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Load the `spaCy` french pre-trained language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_parser = spacy.load('fr_core_news_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Run the NER pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = spacy_parser(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Show the named entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Display the named entities with `displaCy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc, style=\"ent\", jupyter=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Stanza\n",
    "\n",
    "[Stanza](https://stanfordnlp.github.io/stanza/) is another NLP ML library developed by Stanford that is designed to work across many languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Import the `Stanza` library and download the pre-trained french language model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "# This can take a while depending on your internet connection (fr model is 572M)\n",
    "stanza.download('fr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Declare the NER pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stanza_parser = stanza.Pipeline(lang='fr', processors='tokenize,ner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Run the NER pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = stanza_parser(content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Show the named entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 6.3 Rule-based vs. machine learning methods\n",
    "\n",
    "\n",
    "<img src=\"img/annotation_schema_guideline.png\" alt=\"annotation schema\" width=\"800px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Geocoding / Toponym resolution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Quick start with the Geocoder class from the `Perdido` library\n",
    "\n",
    "The `Geocoder()` can take several parameters (all optional) such as:\n",
    "1. sources: list of gazetteers (possible values are: 'nominatim' (default), 'geonames', 'whg', 'pleiades', 'ign' (only for France))\n",
    "2. max_rows: maximum number of toponym candidates return by the gazetteer (default = 1)\n",
    "3. country_code: filter the results per country\n",
    "4. bbox: filter the results with a bounding box (http://bboxfinder.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The next cell shows how to create a geocoder and geocode 'Frontignan':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geocoder = Geocoder()\n",
    "doc = geocoder('Frontignan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Show the geojson results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.geojson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Map the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.get_folium_map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Another example with 'Formose':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = dataset.loc[dataset['head'] == 'FORMOSE'].text.item()\n",
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geocoder = Geocoder()\n",
    "doc = geocoder('Formose')\n",
    "doc.get_folium_map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Gazetteer comparison and geocoding parameters\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Geocoding 'Grumentum':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = dataset.loc[dataset['head'] == 'GRUMENTUM'].text.item()\n",
    "content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2.1 Nominatim (OpenStreetMap) - default gazetteer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geocoder = Geocoder(sources=['nominatim'])\n",
    "doc = geocoder('Grumentum')\n",
    "doc.get_folium_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.geojson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2.2 Geonames\n",
    "\n",
    "http://www.geonames.org\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geocoder = Geocoder(sources=['geonames'])\n",
    "doc = geocoder('Grumentum')\n",
    "doc.get_folium_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.geojson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2.2 World Historical Gazetteer\n",
    "\n",
    "\n",
    "https://whgazetteer.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geocoder = Geocoder(sources=['whg'])\n",
    "doc = geocoder('Grumentum')\n",
    "doc.get_folium_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.geojson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/grumento_whg.png\" alt=\"grumento whg\" width=\"400px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2.4 Querying all gazetteers at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geocoder = Geocoder(sources=['nominatim', 'geonames', 'whg'])\n",
    "doc = geocoder('Grumentum')\n",
    "doc.get_folium_map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2.5 Getting more toponym candidates from gazetteers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geocoder = Geocoder(sources=['nominatim', 'geonames', 'whg'], max_rows=20)\n",
    "doc = geocoder('Grumentum')\n",
    "doc.get_folium_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.geojson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2.6 Geoparsing 'Grumentum' article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geoparser = Geoparser(sources=['whg'])\n",
    "doc = geoparser(content)\n",
    "\n",
    "displacy.render(doc.to_spacy_doc(), style=\"ent\", jupyter=True)\n",
    "\n",
    "doc.get_folium_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places = [d.text for d in doc.named_entities if d.tag == 'place' or d.tag == 'unknown']\n",
    "places"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we use directly the `Geocoder` class as we already have done the geotagging and we get the list of named entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geocoder = Geocoder(sources=['nominatim'])\n",
    "doc = geocoder(places)\n",
    "doc.get_folium_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geocoder = Geocoder(sources=['geonames'])\n",
    "doc = geocoder(places)\n",
    "doc.get_folium_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geocoder = Geocoder(sources=['nominatim', 'geonames', 'whg'], max_rows=10)\n",
    "doc = geocoder(places)\n",
    "doc.get_folium_map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting a bounding box to filter results into a specific area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geocoder = Geocoder(sources=['nominatim', 'geonames', 'whg'], max_rows=10, bbox=[13.079224,38.856820,18.753662,41.037931])\n",
    "doc = geocoder(places)\n",
    "doc.get_folium_map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2.7 Geoparsing 'Frontignan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = dataset.loc[dataset['head'] == 'FRONTIGNAN'].text.item()\n",
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geoparser = Geoparser(version=\"Encyclopedie\", sources=['nominatim', 'geonames', 'ign'])\n",
    "doc = geoparser(content)\n",
    "\n",
    "displacy.render(doc.to_spacy_doc(), style=\"ent\", jupyter=True)\n",
    "doc.get_folium_map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One solution for toponym disambiguation is to build cluters based on spatial density and keep only the cluster with the more distinct toponyms.\n",
    "\n",
    "> Moncla, L., Renteria-Agualimpia, W., Nogueras-Iso, J., & Gaio, M. (2014). Geocoding for texts with fine-grain toponyms: an experiment on a geoparsed hiking descriptions corpus. In Proceedings of the 22nd acm sigspatial international conference on advances in geographic information systems (pp. 183-192).\n",
    "\n",
    "Perdido provides this solution with the `cluster_disambiguation()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.cluster_disambiguation(0.4) \n",
    "\n",
    "doc.get_folium_map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Wrapping up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Going further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Extracting metadata and content from XML-TEI\n",
    "\n",
    "Here we assume that we have access to a directory with the corpus of documents. \n",
    "In our case, documents are XML-TEI files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data/EDdA_vol7/' # path of the directory containing the corpus of documents\n",
    "\n",
    "# select one document for testing\n",
    "file = 'volume07-1002.tei' # FRONTIGNAN: https://artflsrv03.uchicago.edu/philologic4/encyclopedie1117/navigate/7/1002/\n",
    "\n",
    "# get the XML-TEI content of the document\n",
    "root = etree.parse(path + file, etree.XMLParser(remove_blank_text=True)).getroot()\n",
    "\n",
    "# print the XML-TEI content\n",
    "print(xml.parseString(etree.tostring(root)).toprettyxml(indent=' ')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we define a function for parsing and extracting metadata and text content from an XML-TEI file.\n",
    "In this example, we only extract from the metadata the normclass (classification of the article, e.g. 'Géographie'), the head (head word of the article), and the author of the article. Then, we also extract the textual content as raw text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataFromEDDATEI(file_path, filename):\n",
    "    file_id = filename[:-4]\n",
    "    d = []\n",
    "    try:\n",
    "        volume = filename[6:8] \n",
    "        number = filename[9:-4] \n",
    "        head = ''\n",
    "        normClass = ''\n",
    "        author = ''\n",
    "        txtContent = ''\n",
    "        root = etree.parse(file_path+filename).getroot()\n",
    "        div1 = root.find('./text/body/div1')\n",
    "        if len(div1):\n",
    "            for elt in div1:\n",
    "                if elt.tag == 'p':\n",
    "                    txtContent += ''.join(elt.itertext())\n",
    "                    txtContent = txtContent.replace('\\n', ' ').strip()\n",
    "                elif elt.tag == 'index':\n",
    "                    if elt.get('type') == 'normclass':\n",
    "                        normClass = elt.get('value')\n",
    "                    if elt.get('type') == 'head':\n",
    "                        head = elt.get('value')\n",
    "                    if elt.get('type') == 'author':\n",
    "                        author = elt.get('value')\n",
    "        d = [filename, volume, number, head, normClass, author, txtContent]\n",
    "    except etree.XMLSyntaxError as e:\n",
    "        pass\n",
    "        #print(filename + ': ' + str(e))\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use this function to get the metadata an XML-TEI file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getDataFromEDDATEI(path, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to easily analyse and use these data we will now load these information about all the documents in our directory into a [Python dataframe](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for doc in os.listdir(path):\n",
    "    if doc[-4:] == '.tei':\n",
    "        data.append(getDataFromEDDATEI(path, doc))\n",
    "df = pd.DataFrame(data, columns=['filename', 'volume', 'number', 'head', 'normClass', 'author', 'txtContent'])\n",
    "df = df.dropna()\n",
    "df = df.sort_values(['volume', 'number']).reset_index(drop = True)\n",
    "\n",
    "df.head(10) # show the 10 first rows of the dataframe\n",
    "#df.tail(10) # show the 10 last rows of the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have access to all the attributs and methods of the dataframe object. For instance, we can easily print the number of rows in our dataframe which correspond to the number of articles in our corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = df.shape[0]\n",
    "print('There are ' + str(n) + ' articles in the input directory')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Searching by metadata\n",
    "\n",
    "We can select articles based on their classification in the *Encyclopédie*. (There are actually a few different ways that the ARTFL *Encyclopédie* articles have been classified. In this notebook we will be using the `normclass` field, which normalizes classifications given at time of publication that had many spelling variants).\n",
    "\n",
    "If we want all articles classified as 'Geography' we can make the request as follows (the output is stored as a new data frame `df_geo`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "req = 'Géographie'\n",
    "df_geo = dataset[dataset['normClass'].str.contains(req, case=False)]\n",
    "\n",
    "n = df_geo.shape[0]\n",
    "print('There are ' + str(n) + ' geography articles ('+ req +')')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can query based on any value in the dataframe (e.g. article metadata). For instance, we can query all the articles written by a specific author:\n",
    "\n",
    "* Count article for a single named author (Jaucourt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = 'Jaucourt'\n",
    "n = df_geo.loc[dataset['author'] == val].shape[0]\n",
    "print(str(n) + ' were written by '+ val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also easily show the number of articles per author:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geo.groupby(['author'])[\"filename\"].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to show the value of one column in our dataframe for a specific row (i.e., by article) based on its name. For instance, if we want to know who wrote the article about Frontignan or if we want to see its content, we make these requests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.loc[dataset['head'] == 'FRONTIGNAN'].author.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also perform a **keyword search** over the text content of all articles:\n",
    "\n",
    "* Select articles that contain 'france':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search corpus by keyword (val)\n",
    "val = 'france'\n",
    "df_2 = dataset[dataset['text'].str.contains(val, case=False)]\n",
    "print(str(df_2.shape[0]) + ' articles contain the word \\''+ val + '\\'')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to search by **phrases**. The expression \"ville de\" is commonly used in the *Encyclopédie* to define the country or region of a place. Searching by this phrase gives us a sense of the broader geographical coverage of the corpus. \n",
    "\n",
    "Here we extract all articles that contain the expression 'ville de':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[dataset['text'].str.contains(\"ville de\", case=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Processing several documents at once\n",
    "\n",
    "Usually, we want to process a sample of documents, not just one. \n",
    "\n",
    "As the process can be time consuming we will first select a small sample from our dataset to show how it works.\n",
    "\n",
    "* We can use the [sample()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sample.html) method from the pandas library to select randomly a small amount of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sampled = dataset.sample(3)\n",
    "df_sampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Then, we keep only the text content of those documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = df_sampled.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`geoparser` can parse a `string`, a `list` of string or a `pandas.Series`.\n",
    "When the argument is a `list` or a `pandas.series`, the geoparser returns a `PerdidoCollection` object, while when it is a `string` it returns a `Perdido` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = geoparser(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in docs:\n",
    "    print('-----')\n",
    "    displacy.render(doc.to_spacy_doc(), style=\"ent\", jupyter=True) "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Geoparsing.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "49781ad1735bc90a6de953cd911de16c38c0dc835751caa26878ae3207769e1b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
